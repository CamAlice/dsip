{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opentargets in c:\\program files\\python37\\lib\\site-packages (3.1.16)\n",
      "Requirement already satisfied: future==0.16.0 in c:\\program files\\python37\\lib\\site-packages (from opentargets) (0.16.0)\n",
      "Requirement already satisfied: requests<3.0 in c:\\program files\\python37\\lib\\site-packages (from opentargets) (2.23.0)\n",
      "Requirement already satisfied: addict in c:\\program files\\python37\\lib\\site-packages (from opentargets) (2.2.1)\n",
      "Requirement already satisfied: PyYAML in c:\\program files\\python37\\lib\\site-packages (from opentargets) (5.3.1)\n",
      "Requirement already satisfied: cachecontrol==0.11.6 in c:\\program files\\python37\\lib\\site-packages (from opentargets) (0.11.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0->opentargets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0->opentargets) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0->opentargets) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0->opentargets) (2.9)\n",
      "Requirement already satisfied: scispacy in c:\\program files\\python37\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: spacy>=2.2.1 in c:\\program files\\python37\\lib\\site-packages (from scispacy) (2.2.4)\n",
      "Requirement already satisfied: awscli in c:\\program files\\python37\\lib\\site-packages (from scispacy) (1.18.51)\n",
      "Requirement already satisfied: conllu in c:\\program files\\python37\\lib\\site-packages (from scispacy) (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python37\\lib\\site-packages (from scispacy) (1.18.1)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python37\\lib\\site-packages (from scispacy) (0.14.1)\n",
      "Requirement already satisfied: nmslib>=1.7.3.6 in c:\\program files\\python37\\lib\\site-packages (from scispacy) (2.0.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\program files\\python37\\lib\\site-packages (from scispacy) (0.22.1)\n",
      "Requirement already satisfied: pysbd in c:\\program files\\python37\\lib\\site-packages (from scispacy) (0.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (2.23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (4.45.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (41.2.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->scispacy) (1.0.2)\n",
      "Requirement already satisfied: botocore==1.16.1 in c:\\program files\\python37\\lib\\site-packages (from awscli->scispacy) (1.16.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\program files\\python37\\lib\\site-packages (from awscli->scispacy) (0.15.2)\n",
      "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in c:\\program files\\python37\\lib\\site-packages (from awscli->scispacy) (3.4.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\program files\\python37\\lib\\site-packages (from awscli->scispacy) (0.3.3)\n",
      "Requirement already satisfied: PyYAML<5.4,>=3.10 in c:\\program files\\python37\\lib\\site-packages (from awscli->scispacy) (5.3.1)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in c:\\program files\\python37\\lib\\site-packages (from awscli->scispacy) (0.4.3)\n",
      "Requirement already satisfied: psutil in c:\\program files\\python37\\lib\\site-packages (from nmslib>=1.7.3.6->scispacy) (5.7.0)\n",
      "Requirement already satisfied: pybind11>=2.2.3 in c:\\program files\\python37\\lib\\site-packages (from nmslib>=1.7.3.6->scispacy) (2.5.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\program files\\python37\\lib\\site-packages (from scikit-learn>=0.20.3->scispacy) (1.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (2020.4.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\program files\\python37\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (1.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\program files\\python37\\lib\\site-packages (from botocore==1.16.1->awscli->scispacy) (0.9.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\program files\\python37\\lib\\site-packages (from botocore==1.16.1->awscli->scispacy) (2.8.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\program files\\python37\\lib\\site-packages (from rsa<=3.5.0,>=3.1.2->awscli->scispacy) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files\\python37\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python37\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.16.1->awscli->scispacy) (1.14.0)\n",
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz (70.1 MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): en-ner-bionlp13cg-md==0.2.4 from https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz in c:\\program files\\python37\\lib\\site-packages\n",
      "Requirement already satisfied: spacy>=2.2.1 in c:\\program files\\python37\\lib\\site-packages (from en-ner-bionlp13cg-md==0.2.4) (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (4.45.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.18.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (3.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (41.2.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\program files\\python37\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files\\python37\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bionlp13cg-md==0.2.4) (3.0.0)\n",
      "Building wheels for collected packages: en-ner-bionlp13cg-md\n",
      "  Building wheel for en-ner-bionlp13cg-md (setup.py): started\n",
      "  Building wheel for en-ner-bionlp13cg-md (setup.py): finished with status 'done'\n",
      "  Created wheel for en-ner-bionlp13cg-md: filename=en_ner_bionlp13cg_md-0.2.4-py3-none-any.whl size=70542694 sha256=8bb6ac0f580bdcc40205f7278501b78bee1d731ac9af3fb79608235aa8e2b0aa\n",
      "  Stored in directory: c:\\users\\jules\\appdata\\local\\pip\\cache\\wheels\\50\\b2\\7d\\53cff131cb37c8b0197b02f45eb827ff4bb00b119c3a591b4d\n",
      "Successfully built en-ner-bionlp13cg-md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz (70.1 MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): en-ner-bc5cdr-md==0.2.4 from https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz in c:\\program files\\python37\\lib\\site-packages\n",
      "Requirement already satisfied: spacy>=2.2.1 in c:\\program files\\python37\\lib\\site-packages (from en-ner-bc5cdr-md==0.2.4) (2.2.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (4.45.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (0.4.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (41.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (0.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\program files\\python37\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.25.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files\\python37\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (3.0.0)\n",
      "Building wheels for collected packages: en-ner-bc5cdr-md\n",
      "  Building wheel for en-ner-bc5cdr-md (setup.py): started\n",
      "  Building wheel for en-ner-bc5cdr-md (setup.py): finished with status 'done'\n",
      "  Created wheel for en-ner-bc5cdr-md: filename=en_ner_bc5cdr_md-0.2.4-py3-none-any.whl size=70531471 sha256=20ff6e075244dada02053683751e3d27e09004462814cb08ab73bd98fa675132\n",
      "  Stored in directory: c:\\users\\jules\\appdata\\local\\pip\\cache\\wheels\\7c\\f4\\2d\\75a2d2f28a86df956116d40993f5f81df5f5522665c89230eb\n",
      "Successfully built en-ner-bc5cdr-md\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "!pip install opentargets\n",
    "from opentargets import OpenTargetsClient\n",
    "!pip install scispacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_annotations():\n",
    "    \"\"\"\n",
    "    This function looks at all the papers in the CORD-19 dataset and extract entities\n",
    "    \"\"\"\n",
    "    # Define the list of papers we will process\n",
    "    #papers = [Path(\"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/59eab95c43fdea01481fdbf9bae45dfe28ffc693.json\")]\n",
    "    papers = [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/*.json')]\n",
    "    #papers += [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('comm_use_subset/comm_use_subset/pdf_json/*.json')]\n",
    "    #papers += [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('noncomm_use_subset/noncomm_use_subset/pdf_json/*.json')]\n",
    "    #papers += [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('custom_license/custom_license/pdf_json/*.json')]\n",
    "    print (len(papers)) \n",
    "\n",
    "    # Load the NLP models\n",
    "    nlp_model_bionlp13cg = spacy.load('/opt/conda/lib/python3.6/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.4') # For cells, genes, ...\n",
    "    nlp_model_bc5cdr = spacy.load('/opt/conda/lib/python3.6/site-packages/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4') # For diseases\n",
    "\n",
    "    # The output will be one hashmap associating each paper to its annotations\n",
    "    output = {}\n",
    "\n",
    "    # Process all the papers\n",
    "    for paper in tqdm(papers):\n",
    "        try:\n",
    "            # Load the document\n",
    "            document = json.loads(paper.read_text())\n",
    "\n",
    "            # Get the ID\n",
    "            paper_id = document['paper_id']\n",
    "            \n",
    "            # Initialise its entry\n",
    "            output[paper_id] = {}\n",
    "            output[paper_id]['topics'] = {} # The different topic annotations grouped per type\n",
    "            \n",
    "            # Group the text by sections (took more than 9h to process!)\n",
    "            #section_texts = {}\n",
    "            #section_texts['abstract'] = []\n",
    "            #for b in document['abstract']:\n",
    "            #    section_texts['abstract'].append(b['text'])\n",
    "            #for b in document['body_text']:\n",
    "            #    section_texts.setdefault(b['section'], [])\n",
    "            #    section_texts[b['section']].append(b['text'])\n",
    "\n",
    "            # Retrieve all the text\n",
    "            texts = []\n",
    "            for b in document['abstract']:\n",
    "                texts.append(b['text'])\n",
    "            if 'body_text' in document:\n",
    "                for b in document['body_text']:\n",
    "                    texts.append(b['text'])\n",
    "            \n",
    "            # Process the different sections to extract entities\n",
    "            #for section,texts in section_texts.items():\n",
    "            text = '.'.join(texts)\n",
    "            for nlp_model in [nlp_model_bionlp13cg, nlp_model_bc5cdr]:\n",
    "                tokens = nlp_model(text)\n",
    "                for entity in tokens.ents:\n",
    "                    topic_type = entity.label_\n",
    "                    topic_value = str(entity.text)\n",
    "                    output[paper_id]['topics'].setdefault(topic_type, set())\n",
    "                    output[paper_id]['topics'][topic_type].add(topic_value)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print ('Error with {}'.format(paper))\n",
    "            print (e)\n",
    "\n",
    "    # Turn the sets into lists to save them as JSON\n",
    "    for paper_id in output.keys():\n",
    "        for topic_type in output[paper_id]['topics'].keys():\n",
    "            output[paper_id]['topics'][topic_type] = list(output[paper_id]['topics'][topic_type])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9848cddbca47569d48845ae8e741ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Step 1 => get the keywords out of the paper abstract and content\n",
    "annotations = extract_paper_annotations()\n",
    "print (len(annotations.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_annotations_graph(annotations):\n",
    "    \"\"\"\n",
    "    This function is used to generate a graph from the paper annotations\n",
    "    \n",
    "    We will turn all the NLP annotations into concept identifiers using a list of terms extracted form Open Targets and the ontology MONDO. \n",
    "    This is done using a basic exact string matching and all the non matching strings are ignored.\n",
    "\n",
    "    We extract a mapping \"disease name\" => \"disease identifier\" from Open Targets as the primary source, falling back on Mondo to fill the gaps. \n",
    "    In particular one of the missing value in Open Targets right now is Covid-19 ... ;-)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare a map to deal with all the different types of entities type recognized by Spacy and that may be found in the annotations\n",
    "    ontology_map = {\n",
    "        'DISEASE': {},\n",
    "        'CANCER': {},\n",
    "        'GENE_OR_GENE_PRODUCT': {}\n",
    "    }\n",
    "\n",
    "    # TODO: If we want to keep more of the annotations returned by Spacy we should align:\n",
    "    # From https://allenai.github.io/scispacy/ en_ner_bionlp13cg_md\n",
    "    #  CANCER, ORGAN, TISSUE, ORGANISM, CELL, AMINO_ACID, GENE_OR_GENE_PRODUCT, \n",
    "    #  SIMPLE_CHEMICAL, ANATOMICAL_SYSTEM, IMMATERIAL_ANATOMICAL_ENTITY, \n",
    "    #  MULTI-TISSUE_STRUCTURE, DEVELOPING_ANATOMICAL_STRUCTURE, \n",
    "    #  ORGANISM_SUBDIVISION, CELLULAR_COMPONENT\n",
    "    # From https://allenai.github.io/scispacy/ en_ner_bc5cdr_md\n",
    "    #  DISEASE, CHEMICAL\n",
    "\n",
    "    ########################\n",
    "    # Get mappings for DISEASE\n",
    "    ########################\n",
    "    \n",
    "    # Load the file from Open Targets and fill the hashmap in\n",
    "    disease_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz', compression='gzip')\n",
    "    disease_list['disease_full_name'] = disease_list['disease_full_name'].str.lower()\n",
    "    print('Number of keywords in open targets:', len(set(disease_list['disease_full_name'].values)))\n",
    "    for index, row in disease_list.iterrows():\n",
    "        full_name = row['disease_full_name'].lower()\n",
    "        identifier = row['efo_id']\n",
    "        ontology_map['DISEASE'][full_name] = identifier\n",
    "\n",
    "    # Open Targets does not have Covid-19 in its list of diseases. We had it manually\n",
    "    # To get the labels we ran the following query on http://www.ontobee.org/sparql\n",
    "    #  select distinct ?s ?o where {\n",
    "    #    {<http://purl.obolibrary.org/obo/MONDO_0100096> <http://www.geneontology.org/formats/oboInOwl#hasExactSynonym> ?o} \n",
    "    #    union\n",
    "    #    {<http://purl.obolibrary.org/obo/MONDO_0100096> <http://www.w3.org/2000/01/rdf-schema#label> ?o}\n",
    "    #  }\n",
    "    ontology_map['DISEASE']['2019 novel coronavirus infection'.lower()] = 'MONDO_0100096'\n",
    "    ontology_map['DISEASE']['2019-nCoV infection'.lower()] = 'MONDO_0100096'\n",
    "    ontology_map['DISEASE']['severe acute respiratory syndrome coronavirus 2'.lower()] = 'MONDO_0100096'\n",
    "    ontology_map['DISEASE']['SARS-CoV-2'.lower()] = 'MONDO_0100096'\n",
    "    ontology_map['DISEASE']['SARS-coronavirus 2'.lower()] = 'MONDO_0100096'\n",
    "    ontology_map['DISEASE']['coronavirus disease 2019'.lower()] = 'MONDO_0100096'\n",
    "    ontology_map['DISEASE']['COVID-19'.lower()] = 'MONDO_0100096'\n",
    "    \n",
    "    # Debug output\n",
    "    print ('Number of keywords in map for diseases: {}'.format(len(ontology_map['DISEASE'])))\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # Get mappings for CANCER\n",
    "    ########################\n",
    "\n",
    "    # We will simply treat the \"CANCER\" annotations from Spacy as \"DISEASE\"\n",
    "    ontology_map['CANCER'] = ontology_map['DISEASE']\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # Get mappings for GENE_OR_GENE_PRODUCT\n",
    "    ########################\n",
    "    \n",
    "    # Load a target list from Open Targets. It will be used to map gene keywords\n",
    "    target_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_target_list.csv.gz', compression='gzip')\n",
    "    target_list['hgnc_approved_symbol'] = target_list['hgnc_approved_symbol'].str.lower()\n",
    "    print('Number of genes in open targets:', target_list['hgnc_approved_symbol'].nunique())\n",
    "    for index, row in target_list.iterrows():\n",
    "        full_name = row['hgnc_approved_symbol']\n",
    "        identifier = row['ensembl_id']\n",
    "        ontology_map['GENE_OR_GENE_PRODUCT'][full_name] = identifier\n",
    "    \n",
    "\n",
    "    ########################\n",
    "    # Turn the paper annotations into a graph\n",
    "    ########################\n",
    "    graph = []\n",
    "    predicates = {\n",
    "        'DISEASE': 'isAboutDisease',\n",
    "        'CANCER': 'isAboutDisease',\n",
    "        'GENE_OR_GENE_PRODUCT': 'isAboutTarget'\n",
    "    }\n",
    "    # Go through all the papers\n",
    "    for (paper_id, data) in annotations.items():\n",
    "        # For each annotation topic try to find a match in the ontology map\n",
    "        for (topic, values) in data['topics'].items():\n",
    "            if topic in ontology_map:\n",
    "                for value in values:\n",
    "                    if value.lower() in ontology_map[topic]:\n",
    "                        obj = ontology_map[topic][value.lower()]\n",
    "                        graph.append([paper_id, predicates[topic], obj])\n",
    "               \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_targets_and_diseases(graph):\n",
    "    \"\"\"\n",
    "    This function will use the association data from Open Target to \n",
    "    connect instances of Target and Disease in the graph.\n",
    "    \n",
    "    We at the same time connect diseases to therapeutic areas (instances of Disease)\n",
    "    as this information is returned by the API\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a list of all the targets (genes) and diseases currently in the graph\n",
    "    targets = list(set([t[2] for t in graph if t[1] == 'isAboutTarget']))\n",
    "    diseases = list(set([t[2] for t in graph if t[1] == 'isAboutDisease']))\n",
    "    \n",
    "    # Prepare a map of target => disease relations\n",
    "    ot_output_associations = {}\n",
    "    \n",
    "    # Query OpenTargets for Target => Disease associations\n",
    "    ot = OpenTargetsClient()\n",
    "    for target in tqdm(targets):\n",
    "        ot_output_associations.setdefault(target, set())\n",
    "        search_results = ot.get_associations_for_target(target)\n",
    "        if len(search_results) > 0 and search_results[0]['target']['id'] == target:\n",
    "            for search_result in search_results:\n",
    "                if search_result['association_score']['overall'] > 0.8:\n",
    "                    disease = search_result['disease']['id']\n",
    "                    ot_output_associations[target].add(disease)\n",
    "                        \n",
    "    # Query OpenTargets for Disease => Target associations\n",
    "    for disease in tqdm(diseases): \n",
    "        search_results = ot.get_associations_for_disease(disease)\n",
    "        if len(search_results) > 0 and search_results[0]['disease']['id'] == disease:\n",
    "            for search_result in search_results:\n",
    "                if search_result['association_score']['overall'] > 0.8:\n",
    "                    target = search_result['target']['id']\n",
    "                    ot_output_associations.setdefault(target, set())\n",
    "                    ot_output_associations[target].add(disease)\n",
    "\n",
    "    # Turn the output into new edges in the graph\n",
    "    for target, diseases in ot_output_associations.items():\n",
    "        for disease in diseases:\n",
    "            # Target -> Disease relation\n",
    "            graph.append([target, 'isAssociatedTo', disease]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_diseases_to_diseases(graph):\n",
    "    \"\"\"\n",
    "    This function leverages the disease similarity information computed by Open Targets\n",
    "    to connect Diseases to each other. Those links will later be used to find risk factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a list of all the diseases currently in the graph.\n",
    "    # We do that by looking at the objects of triples we know link to Diseases\n",
    "    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n",
    "    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo']) \n",
    "    \n",
    "    # Query OpenTargets\n",
    "    ot_output_diseases = {}\n",
    "    ot = OpenTargetsClient()\n",
    "    for disease in tqdm(diseases):\n",
    "        ot_output_diseases[disease] = set()\n",
    "        search_results = ot.get_similar_disease(disease)\n",
    "        for search_result in search_results:\n",
    "            if search_result['subject']['id'] == disease: # Safe guard\n",
    "                ot_output_diseases[disease].add(search_result['object']['id'])\n",
    "                \n",
    "    # Turn the output we received into edges\n",
    "    for src_disease, target_diseases in ot_output_diseases.items():\n",
    "        for target_disease in target_diseases:\n",
    "            graph.append([src_disease, 'hasGeneticClue', target_disease])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_disease_classification(graph):\n",
    "    \"\"\"\n",
    "    This function adds to the graph the disease classification tree.\n",
    "    See, for example, https://www.targetvalidation.org/disease/EFO_0005774 .\n",
    "    \"\"\"\n",
    "\n",
    "    # Get a list of all the diseases in the graph\n",
    "    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n",
    "    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo']) \n",
    "    diseases = diseases | set([t[2] for t in graph if t[1] == 'hasGeneticClue']) \n",
    "\n",
    "    # Query OpenTargets\n",
    "    paths = set()\n",
    "    ot = OpenTargetsClient()\n",
    "    for disease in tqdm(diseases):\n",
    "        search_results = ot.search(disease)\n",
    "        if search_results != None and len(search_results) > 0:\n",
    "            search_result = search_results[0]\n",
    "            if search_result['id'] == disease:\n",
    "                if 'efo_path_codes' in search_result['data']:\n",
    "                    for path in search_result['data']['efo_path_codes']:\n",
    "                        paths.add('=>'.join(path))\n",
    "                        \n",
    "    # Turn the output we received into edges\n",
    "    for path_str in paths:\n",
    "        path = path_str.split('=>')\n",
    "        for index in range(0, len(path)-1):\n",
    "            start = path[index]\n",
    "            end = path[index+1]\n",
    "            graph.append([end, 'isASpecific', start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_disease_therapeutic_areas(graph):\n",
    "    \"\"\"\n",
    "    This function query Open Targets for the therapeutic area of all the diseases\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a list of all the diseases in the graph\n",
    "    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n",
    "    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo'])\n",
    "    diseases = diseases | set([t[2] for t in graph if t[1] == 'hasGeneticClue']) \n",
    "    diseases = diseases | set([t[2] for t in graph if t[1] == 'isASpecific']) \n",
    "    \n",
    "    # Query OpenTargets\n",
    "    ot_output = {}\n",
    "    ot = OpenTargetsClient()\n",
    "    for disease in tqdm(diseases):\n",
    "        ot_output[disease] = set()\n",
    "        search_results = ot.get_disease(disease)\n",
    "        if search_results != None and len(search_results) > 0:\n",
    "            search_result = search_results[0]\n",
    "            if search_result['code'].endswith(disease) and 'therapeutic_codes' in search_result:\n",
    "                for therapeutic_code in search_result['therapeutic_codes']:\n",
    "                        ot_output[disease].add(therapeutic_code)\n",
    "                        \n",
    "    # Turn the output we received into edges\n",
    "    for (disease, areas) in ot_output.items():\n",
    "        for area in areas:\n",
    "            graph.append([disease, 'belongsToTherapeuticArea', area])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_stats(graph):\n",
    "    resources = set([r[0] for r in graph]) | set([r[2] for r in graph])\n",
    "    predicates = set([r[1] for r in graph])\n",
    "    print ('Graph has {} edges, {} resources, {} predicates'.format(len(graph), len(resources), len(predicates)))\n",
    "    display(pd.DataFrame([t for t in graph], columns=['Subject', 'Predicate', 'Object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 => get the starting graph of paper annotations\n",
    "graph = get_paper_annotations_graph(annotations)\n",
    "print_graph_stats(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 => enrich the graph with Target - Disease links\n",
    "connect_targets_and_diseases(graph)\n",
    "print_graph_stats(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 => connect diseases to related diseases\n",
    "connect_diseases_to_diseases(graph)\n",
    "print_graph_stats(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 => add disease classification trees\n",
    "add_disease_classification(graph)\n",
    "print_graph_stats(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 => add disease therapeutic areas\n",
    "add_disease_therapeutic_areas(graph)\n",
    "print_graph_stats(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we do a last pass to remove duplicate statements\n",
    "final_graph = [t.split('=>') for t in set(['=>'.join(t) for t in graph])]\n",
    "print_graph_stats(final_graph)\n",
    "\n",
    "# and we save the graph to disk\n",
    "graph_df = pd.DataFrame(final_graph, columns=['subject', 'predicate', 'object'])\n",
    "graph_df.to_csv('graph.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(resource):\n",
    "    # Extract a disease and target code=>label\n",
    "    to_name = {}\n",
    "    target_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_target_list.csv.gz', compression='gzip')\n",
    "    for row in target_list.itertuples():\n",
    "        to_name[row.ensembl_id] = row.hgnc_approved_symbol\n",
    "    disease_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz', compression='gzip')\n",
    "    for row in disease_list.itertuples():\n",
    "        to_name[row.efo_id] = row.disease_full_name\n",
    "    \n",
    "    # Extract edges we may be interested in\n",
    "    triples = [t for t in final_graph if t[0] == resource or t[2] == resource]\n",
    "\n",
    "    # Construct a dataframe\n",
    "    tmp = []\n",
    "    for t in triples:\n",
    "        s = '{} ({})'.format(t[0], to_name.get(t[0], '?'))\n",
    "        o = '{} ({})'.format(t[2], to_name.get(t[2], '?'))\n",
    "        tmp.append([s,t[1],o])\n",
    "        \n",
    "    return pd.DataFrame(tmp, columns=['Subject', 'Predicate', 'Object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_neighbours('MONDO_0008903'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_neighbours('MONDO_0100096'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(117)\n",
    "from ampligraph.latent_features import ComplEx, TransE, DistMult, RandomBaseline\n",
    "\n",
    "from ampligraph.evaluation import evaluate_performance, mrr_score, hits_at_n_score, mr_score\n",
    "from ampligraph.utils import save_model, restore_model\n",
    "\n",
    "\n",
    "DATASET_BASE_PATH = \"/kaggle/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327834\n",
      "Size of the graph: (327834, 3)\n",
      "Index(['subject', 'predicate', 'object'], dtype='object')\n",
      "isAssociatedTo              176088\n",
      "hasGeneticClue              103103\n",
      "belongsToTherapeuticArea     22142\n",
      "isASpecific                  17548\n",
      "isAboutDisease                5986\n",
      "isAboutTarget                 2967\n",
      "Name: predicate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "triples = pd.read_csv(\"graph.csv\")\n",
    "\n",
    "paper_diseases = set(triples[triples.predicate == 'isAboutDisease'].object)\n",
    "paper_targets = set(triples[triples.predicate == 'isAboutTarget'].object)\n",
    "new_triples = []\n",
    "for row in triples.itertuples():\n",
    "    if row.predicate == 'isAssociatedTo':\n",
    "        if row.subject in paper_targets or row.object in paper_diseases:\n",
    "            new_triples.append([row.subject, row.predicate, row.object])\n",
    "    if row.predicate == 'isAboutDisease' or row.predicate == 'isAboutTarget':\n",
    "        new_triples.append([row.subject, row.predicate, row.object])\n",
    "    if row.predicate == 'hasGeneticClue':\n",
    "        if row.subject in paper_diseases or row.object in paper_diseases:\n",
    "            new_triples.append([row.subject, row.predicate, row.object])\n",
    "    if row.predicate == 'isASpecific':\n",
    "        new_triples.append([row.subject, row.predicate, row.object])\n",
    "    if row.predicate == 'belongsToTherapeuticArea':\n",
    "        new_triples.append([row.subject, row.predicate, row.object])\n",
    "print (len(new_triples))\n",
    "\n",
    "\n",
    "graph_df = pd.DataFrame(new_triples, columns=['subject', 'predicate', 'object'])\n",
    "\n",
    "# this line is added for making sure that the results are reproducible.\n",
    "graph_df.sort_values(by=['subject', 'predicate', 'object'], inplace=True)\n",
    "\n",
    "graph_df.to_csv('COVID_KG_sample.csv', index=False)\n",
    "\n",
    "graph_df.head()\n",
    "\n",
    "print('Size of the graph:', graph_df.shape)\n",
    "\n",
    "print(graph_df.columns)\n",
    "\n",
    "print(graph_df.predicate.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_clue_triples = graph_df[graph_df['predicate']=='hasGeneticClue']\n",
    "train_set = graph_df[graph_df['predicate']!='hasGeneticClue'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diseases in df: 5115\n"
     ]
    }
   ],
   "source": [
    "disease_list =  np.unique(np.concatenate([\n",
    "                    np.unique(train_set[train_set[:, 1]=='isAboutDisease'][:, 2]),\n",
    "                    np.unique(train_set[train_set[:, 1]=='isAssociatedTo'][:, 2]),\n",
    "                ], 0))\n",
    "\n",
    "print('diseases in df:', len(disease_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Orphanet_1333', 'EFO_0008491', 'MONDO_0000879', 'Orphanet_166100', 'EFO_0003032', 'EFO_0008238', 'MONDO_0004379', 'HP_0000713', 'Orphanet_209188', 'Orphanet_158029', 'EFO_0003085', 'MONDO_0021110', 'EFO_1000603', 'Orphanet_314', 'EFO_0005762', 'MONDO_0017595', 'Orphanet_98523', 'MONDO_0016054', 'HP_0012167', 'EFO_0000760', 'EFO_1001380', 'MONDO_0000540', 'EFO_0009567', 'EFO_1001821', 'EFO_0000182', 'MONDO_0020639', 'EFO_0006514', 'EFO_0003768', 'MONDO_0024255', 'EFO_0009449', 'Orphanet_181393', 'Orphanet_2554', 'EFO_0007141', 'Orphanet_181412', 'Orphanet_98620', 'MONDO_0021463', 'Orphanet_206656', 'MONDO_0002542', 'MONDO_0007263', 'MONDO_0021092', 'EFO_0010228', 'Orphanet_2116', 'Orphanet_98054', 'Orphanet_79201', 'EFO_0003824', 'EFO_0010088', 'Orphanet_247820', 'EFO_0005127', 'EFO_1000204', 'EFO_1001875', 'EFO_0004641', 'Orphanet_3421', 'MONDO_0100070', 'EFO_0003047', 'EFO_0008500', 'EFO_0009641', 'Orphanet_1195', 'Orphanet_209978', 'EFO_0005251', 'MONDO_0037940', 'EFO_1000348', 'MONDO_0015937', 'Orphanet_79364', 'EFO_0005593', 'HP_0000011', 'Orphanet_1540', 'EFO_1001069', 'HP_0011024', 'Orphanet_2098', 'Orphanet_79506', 'EFO_0009078', 'EFO_1000601', 'Orphanet_93173', 'EFO_1000107', 'Orphanet_165661', 'Orphanet_52430', 'EFO_0009104', 'EFO_0005105', 'MONDO_0014260', 'MONDO_0021348', 'EFO_0004701', 'EFO_0007042', 'Orphanet_749', 'EFO_0006845', 'EFO_0005753', 'EFO_0006740', 'Orphanet_93449', 'EFO_0000333', 'EFO_0000705', 'EFO_0000658', 'MONDO_0007576', 'EFO_0006331', 'Orphanet_156215', 'MONDO_0016685', 'Orphanet_2833', 'HP_0030052', 'HP_0001263', 'Orphanet_178338', 'MONDO_0021312', 'MONDO_0018859']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(117)\n",
    "\n",
    "test_set_diseases = np.random.choice(list(disease_list), 100).tolist()\n",
    "\n",
    "#test_set_diseases = set(np.random.choice(list(disease_list), 2).tolist())\n",
    "print(test_set_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (325382, 3)\n",
      "Test set size: (2452, 3)\n",
      "Full Graph size: (327834, 3)\n"
     ]
    }
   ],
   "source": [
    "test_set = genetic_clue_triples[genetic_clue_triples[\"subject\"].isin(test_set_diseases)]\n",
    "train_genetic_clue_triples = genetic_clue_triples[~genetic_clue_triples[\"subject\"].isin(test_set_diseases)]\n",
    "train_set = np.concatenate([train_set, train_genetic_clue_triples], 0)\n",
    "train_set = np.random.permutation(train_set)\n",
    "\n",
    "print('Train set size:', train_set.shape)\n",
    "print('Test set size:', test_set.shape)\n",
    "print('Full Graph size:', graph_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diseases in df: 7218\n"
     ]
    }
   ],
   "source": [
    "disease_list_full =  np.unique(np.concatenate([\n",
    "                        np.unique(train_set[train_set[:, 1]=='isAboutDisease'][:, 2]),\n",
    "                        np.unique(train_set[train_set[:, 1]=='isAssociatedTo'][:, 2]),\n",
    "                        np.unique(train_set[train_set[:, 1]=='hasGeneticClue'][:, 0]),\n",
    "                        np.unique(train_set[train_set[:, 1]=='hasGeneticClue'][:, 2]),\n",
    "                    ], 0))\n",
    "\n",
    "print('diseases in df:', len(disease_list_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ampligraph\\latent_features\\models\\EmbeddingModel.py:1130: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if corruption_entities == 'all':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR with random baseline: 0.0010271726695613857\n"
     ]
    }
   ],
   "source": [
    "filter_triples = genetic_clue_triples.values\n",
    "\n",
    "random_model = RandomBaseline(seed=0)\n",
    "\n",
    "random_model.fit(train_set)\n",
    "\n",
    "ranks = evaluate_performance(test_set.values, \n",
    "                             random_model, \n",
    "                             filter_triples=filter_triples, \n",
    "                             corrupt_side='o', \n",
    "                             entities_subset=list(disease_list_full))\n",
    "\n",
    "print('MRR with random baseline:', mrr_score(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ampligraph\\latent_features\\models\\EmbeddingModel.py:1130: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if corruption_entities == 'all':\n",
      "Average Loss:   2.547246:   4%|█▉                                            | 42/1000 [1:06:44<26:32:48, 99.76s/epoch]"
     ]
    }
   ],
   "source": [
    "filter_triples = genetic_clue_triples.values\n",
    "\n",
    "\n",
    "model = ComplEx(batches_count=15, seed=0, epochs=1000, k=200, eta=20,\n",
    "                optimizer='adam', optimizer_params={'lr':1e-4}, \n",
    "                verbose=True, loss='multiclass_nll',\n",
    "                regularizer='LP', regularizer_params={'p':3, 'lambda':1e-3})\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = { 'x_valid': test_set.values,\n",
    "                   'criteria': 'mrr', \n",
    "                  'x_filter': filter_triples, \n",
    "                  'stop_interval': 3, \n",
    "                  'burn_in': 50, \n",
    "                  'corrupt_side':'o',\n",
    "                  'corruption_entities': list(disease_list_full),\n",
    "                  'check_interval': 50 }\n",
    "\n",
    "model.fit(train_set, True,early_stopping)\n",
    "\n",
    "ranks = evaluate_performance(test_set.values, \n",
    "                             model, \n",
    "                             filter_triples=filter_triples, \n",
    "                             corrupt_side='o', \n",
    "                             entities_subset=list(disease_list_full))\n",
    "\n",
    "print('MRR with trained ComplEx embedding model:', mrr_score(ranks))\n",
    "\n",
    "model.calibrate(train_set, positive_base_rate=0.5, epochs=100)\n",
    "save_model(model, 'output_graph.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations\n",
    "create_tensorboard_visualizations(model, 'covid19_tensorboard_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_id = 'MONDO_0100096' #covid-19\n",
    "\n",
    "test_predicate = 'hasGeneticClue'\n",
    "\n",
    "hypothesis = np.concatenate([np.array([[disease_id] * disease_list_full.shape[0]]), \n",
    "                             np.array([[test_predicate] * disease_list_full.shape[0]]),\n",
    "                             disease_list_full[np.newaxis, :]],0).T\n",
    "print(hypothesis.shape)\n",
    "\n",
    "scores = model.predict_proba(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_mapping_list_df = disease_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz', \n",
    "                                                     compression='gzip')\n",
    "\n",
    "\n",
    "disease_mapping_list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_hypothesis = pd.DataFrame(np.concatenate([hypothesis, \n",
    "                                                 scores[:, np.newaxis]], 1), \n",
    "                                 columns=['s','p','o','score'])\n",
    "\n",
    "tested_hypothesis = tested_hypothesis[tested_hypothesis['o'] != disease_id]\n",
    "\n",
    "tested_hypothesis = tested_hypothesis.sort_values(by='score', \n",
    "                                                  ascending=False)\n",
    "\n",
    "tested_hypothesis = tested_hypothesis.merge(disease_mapping_list_df, \n",
    "                                            how='left', \n",
    "                                            left_on='o', \n",
    "                                            right_on='efo_id')[['disease_full_name', 'score']]\n",
    "\n",
    "tested_hypothesis.columns = ['Risk Factors', 'Score']\n",
    "\n",
    "pd.set_option('display.max_rows', 101)\n",
    "\n",
    "tested_hypothesis.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_hypothesis.to_csv('predicted_covid19_risk_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
